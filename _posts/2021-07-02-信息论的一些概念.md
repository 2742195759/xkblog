---
layout : post
title  : 贝叶斯神经网络
date   : 2021-07-02 18:00:00 +0000
category : NN
typora-copy-images-to: ../../../code/xkblog/public/img/
typora-root-url: ../../../code
---

这一节主要进行信息论基本概念的总结和与其他知识的链接。



基本记号：

x ,y 是随机变量，p, q是分布。通常情况下q, p是x的分布。

##### 信息熵

$H(p) = \int_x - p(x)\log(p(x)) = \mathbb{E}_{p}[-\log(p(x))]$

##### KLDiv

$KL(p||q) = H(p) - \mathbb{E}_p[-\log(q(x))]$

= $\int_{x} - p(x) \log \frac{p(x)}{q(x)}$

##### 联合熵

H(x, y) = $\int_{x,y} -p(x,y) \log p(x, y)$

##### 条件熵

$H(x | y) = \int_{x,y} -p(x,y) \log p(x|y) = \int_{x,y} -p(x,y) \log \frac{p(x,y)}{p(y)}$

= $H(x,y) - H(y)$ 

 ***即： 条件熵可以理解为观察到Y后，X不确定性的损失***

##### Mutual Information

总的X不确定性H(X)

知道Y后，对X不确定性的影响：

H(X) - H(X|Y) = $\int_x -p(x,y) \log \frac{p(x)p(y)}{p(x,y)}$ = MI(X,Y)

MI具有对称性质。



