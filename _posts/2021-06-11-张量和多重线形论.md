---
layout : post
title  : 张量和多重线形论
date   : 2021-06-12 13:00:00 +0000
category : phraseground
typora-copy-images-to: ../../../code/xkblog/public/img/
typora-root-url: ../../../code
---

乌镇回来，机场航班取消，所以在机场写下这些东西。算是对自己知识的一个巩固。

### 张量和张量积、多重线形论

#### Multi-Linear Theory



##### Linear Function Space

$\sum x_i e_i * \sum y_i e_i = \sum a_{ij} x_i y_j$

满足下列性质的线形函数： $\R^{m} : \R$

$f(x + y) = f(x) + f(y)$

$f(\lambda x) = \lambda f(x)$



推导： 对输入的x进行基代换： 

$x = \sum x_i e_i$ ，则$f(x) = \sum x_i f(e_i)$。则任何一个线形函数都可以由一个向量代替：

<font color='red'>结论1: $存在从 V到V'的典范映射: \theta: v \rightarrow f定义为, f(x) = \sum x_i v_i$ ，即一个向量对应了一个线形函数。点积即函数映射值。</font>



推导：$给定V空间\in \R^{m}, V'空间对偶于 \in \R^m，典范映射是 \theta。由于对偶性质，可以得到对偶空间的基定义为\alpha_i = \theta(e_i)，即\alpha_i(e_i)=1，而\alpha_i(e_j)=0$。



推导：

$\alpha_i(x) = x_i$



推导：对偶空间分解

任意给定给一个$f \in V'$，可以对$f $进行分解，得到由$\alpha_i$的基的和的形式： 令$\theta(f) = \vec f$，则$f = \sum \vec f_i \alpha_i$

证明：

$f(x) = \sum x_i f(e_i) = \sum \vec f_i x_i = \sum \vec f_i \alpha_i (x) = (\sum \vec f_i \alpha_i) (x)$

$\square$



<font color='green'>总结：如果给定一个 $x \in \R^m$  向量，首先可以是 $\R^m$ 空间的一个点也可以是$m维空间的一个线形函数$。相对的，点乘即可以是一个双线形函数，也可以是线形函数的函数映射操作。 </font>



##### Bilinear Function Space

定义：双线形函数

例子：点积函数是典型的双线形函数，类似于乘积

例子：乘法函数 $\R \times \R \rightarrow \R$是最简单的一个双线形函数



推导：对“点积”操作使用基替换，注意这里的基是不同的两个线形空间的dot，不一定是正交基。

$x \cdot y = \sum x_i e_i \cdot \sum y_i e_i = \sum x_i y_j (e_i \cdot e_j)$，令($e_i \cdot e_j$) = $a_{ij}$，则可以有$x \cdot y = \sum x_i y_j a_{ij} = x^TAy$ 

如果是标准正交基，那么A = I ，否则A可以是任何一个正定矩阵。

通过上述过程可以知道，任何线形空间$x，y \in L_1$ ，只要给定了一个Basis=$e_1, ... e_n$，那么就存在一个正定矩阵A对应点积操作。



推广：点积到普通的双线形函数： 

$双线形函数 f(x,y)，x \in L_x , y \in L_y为两个空间，空间基为e_i 和 v_i，则使用基分解可以有如下推导：$

$f(x, y) = f(\sum x_i e_i , \sum y_i v_i) = \sum x_i y_j f(e_i, v_j)$。 令 A 矩阵为 $A_{ij} = f(e_i, v_j)$，则可以得到一个双线形函数和一个A矩阵一一对应。A $\in \R^{m \times m}$ 。



<font color='green'>通过单线形和多线形的推导，可以发现向量到矩阵的推广是单线形 -> 双线形的推广。</font>



推导： 双线形分解？

类似单线性的将$f$分解为$\alpha_i$，是否可以将双线形的操作分解为类似形式$\alpha_{ij}(x,y)$函数呢？

从$\R^{m} \times \R^m \rightarrow \R$的点积开始: 

$x \cdot y = \sum x_i y_j (e_i \cdot v_j) = \sum x_i y_j a_{ij}$ 期中 A矩阵表达了Basis的选择。那么剩下的就是点积的完整表达了。

$\sum x_i y_j a_{ij} = \sum \alpha_i(x) \alpha_j(y) a_{ij}$。定义$\alpha_i(x) * \alpha_j(y) = \alpha_{ij}(x, y)$ ，那么我们得到了点积操作作为双线形函数的分解。

$\sum x_i y_j a_{ij} = \sum a_{ij} \alpha_{ij}(x, y)$



定理：$\R^{m} \times \R^m \rightarrow \R$的点积dot()操作可以分解为 $\mathbf{dot}(x, y) = \sum \alpha_{ij} (x, y)$ ，即$\mathbf{dot} = \sum a_{ij} \alpha_{ij}$。其中$a_{ij}与V的基选择有关$。$\alpha_{ij}与上述定义相同：\alpha_{i,j}(x, y) = x_i * y_j$。



定理：【任何一个双线性函数】

$f(x, y) = f(\sum x_i e_i , \sum y_i v_i) = \sum x_i y_j f(e_i, v_j)$ 令 $f(e_i, v_j)$矩阵为F矩阵 $F_{ij}= f(e_i, v_j)$ 。则有如下变化

$f(x,y) = \sum x_i y_i F_{ij} = \sum \alpha^x_i(x) \alpha^y_j(y) F_{ij}$ 

令 $\alpha^x_i(x) * \alpha^y_i(y) = \alpha^x_i \otimes \alpha^y_j (x, y)$ 

则有 $f(x, y) = (\sum F_{ij} \alpha_i \otimes \alpha_j)(x, y)$

因此有分解定理：
$$
分解定理：任何一个双线形函数可以分解为 f(x, y) = (\sum F_{ij} \alpha_i \otimes \alpha_j)(x, y) 的形式，其中\alpha_i \otimes \alpha_j为双线形空间的基，是一个双线形函数。而F矩阵是f函数的分解量。
$$
其中F是只与f和basis有关的变量。



推导： 【回头看点积】

所以我们可以知道当 $f(x, y)$ 为点积操作，可以得到 $F_{ij} = e_i \cdot e_j$，也就是我们的A矩阵。所以上述的点积是一个特例。满足正定性质。



定义：【张量积】

将一个张量和一个多重线形映射作为一一对应关系：则有

 $F \otimes G$  作为 $f(\mathbf{x}) * g(\mathbf{y})$ 函数的张量。定义符号 $i_{1}^{n} = i_1, i_2, ... i_n$。其中 i 为基的下标。

$f(\mathbf{x}) * g(\mathbf{y}) = \sum F \alpha^f(\mathbf{x}) * \sum G * \alpha^g (\mathbf{x}) = \sum_{i_1^{n+m}} F_{i_{1}^{n}} * G_{i_{n+1}^{n+m}} * \alpha^f \otimes \alpha_g(\mathbf{x}, \mathbf{y})$

而上述式子的系数也就是 $F \otimes G$

因此张量积目前可以定性的定义为 -- 张量积：

 $T \in \R^{a_0, a_1, a_2 ...,a_n} , U \in \R^{b_0, b_1, b_2 ..., b_m}$ 则

$(T \otimes U)_{i_1^{n+m}} = T_{i_i^n} * U_{i_{n+1}^{n+m}} $，其中 $T_{i0,i1,i2,...,in}$表示T的下标选择i0, i1, i2 ... in的数字。



