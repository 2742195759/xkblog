---
layout : post
title  : Visdom-Embedding功能使用
date   : 2021-03-09 13:00:00 +0000
category : cvpack2
typora-copy-images-to: ../../../code/xkblog/public/img/
typora-root-url: ../../../code
---

#### 继承DefaultTrainer实践



#### config的重用与坑点

1. config的时候，必须要使用clone保存一份传入Trainer。否则其中一些参数可能被Trainer改写，导致后续调用会出现和第一次调用不一致的情况。（主要的修改在 `DefaultTrainer.__init__`函数中对`max_epoch`和`max_iter`的改写中。ß）
2. config的时候，如果想要使用cvpack2的args机制，需要`default_setup(conf, args)`函数调用，注意本函数也会改写部分参数。
3. 如果想要使用Distributed训练，一定要调用`default_setup`来进行初始化，否则会出现迭代次数不对的情况。因为这里捕获了一些distribute逻辑。



#### 数据集Shuffle坑点

原始的DistributedGroupSample有坑点，默认的随机种子只与epoch有关系。因此如果多次调用train()函数会导致每次都是同样的数据shuffle。为了引入shuffle性质，需要改为新增的 DistributedGroupSampleTimeSeed 类型。只需要设置config：

```
DATALOADER=dict(
	
)
```

#### SEED的坑点

不同的rank会使用不同的seed。因为set_up的时候，会将seed + rank。因此每个rank都是不同的seed。如果想要保证数据集中的seed一样，最好提前进行预处理，预处理完毕才是王道。

总结，不要使用SEED作为同步不同rank的方法，使用额外的seed参数。例如添加一个 `DATASETS.SEED` 。【已添加】

