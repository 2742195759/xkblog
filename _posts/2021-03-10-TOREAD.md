---
layout : post
title  : TOREAD+PaperPOOL
date   : 2022-03-10 16:00:00 +0000
category : todo
typora-copy-images-to: ../../../code/xkblog/public/img/
typora-root-url: ../../../code
---

#### 深度学习

[介绍CNN的可视化发展过程](https://zhuanlan.zhihu.com/p/24833574)：知乎的介绍CNN卷积核可视化的文章，也许对其他的可视化过程也可以有启发。

#### RE

论文： Inter- pretable and globally optimal prediction for textual ground- ing using image concepts.

## Multi-Task Learning Basis

```mermaid
graph TD;
1[Multi-Task Learning] --problem--- 1.1[compete/任务竞争:<br>]
1 --problem--- 1.2[catastrophic forgetting:<br>如果在multi-task中Early<br>Stopping会导致forget]
1.2 --solved by --- 1.2.1[Dynamic Stop-and-Go/DSG:<br>]
1 --train method --- 1.3[Curriculum Learning/ICML2009<br>]



click 1.1 "https://arxiv.org/abs/1905.07553v1" "论文地址"
click 1.3 "https://cloud.tsinghua.edu.cn/f/e06ab7f76d124578998f/?dl=1" "论文下载 ICML"
```




## Tasks and Methods Related to Referring Expression

```mermaid
graph TD; 
1[Visual and Textual Tasks] --包含--> 1.1[Referring Expression]
1 --包含--> 1.2[Image-Text Matching]
1.2 --方法--> 1.2.1[SCAN / ECCV2018]
2[自监督方法] --视觉文本类--- 1.1.1.1[ViLBERT]
2 --视觉文本类--- 2.1[Uniter/自监督:<br>]
1.1 -- 方法 --- 1.1.1[12-in-1 CVPR2020: <br> ]
1.1.1 -- 基于 --> 1.1.1.1[ViLBERT/CVPR2019]
1.1 -- 方法 --- 1.1.2[sdf]





click 1.1.1 "https://openaccess.thecvf.com/content_CVPR_2020/html/Lu_12-in-1_Multi-Task_Vision_and_Language_Representation_Learning_CVPR_2020_paper.html" "CVPR2020"

click 1.1.1.1 "#vilbert" "pdflink"

click 2.1 "https://arxiv.org/abs/1909.11740" "pdflink"

click 1.2.1 "#scan" "pagelink"
```

##### CVPR2020:12-in-1

1. 最主要的创新是啥：引入了Multi-Task Learning。共享Emb的方式来建立Task的连接。

2. 基于的模型和论文：<span id='vilbert'> ViLBERT </span> [论文地址](https://arxiv.org/abs/1908.02265) CVPR2019

   总而言之，ViLBERT是一个文本和视觉的联合自监督预训练网络。使用2个并行TRM进行Inner-Modality特征提取。同时还有Co-TRM进行Inter-Modality提取。输入为IMG R0 R1 .. Rn CLS w0 w1 ... wm SEP。其中IMG和CLS作为视觉和文本两个Emb。2个自监督任务(proxy tasks)：Multi-Modal alignment prediction / Masked Multi-Modal Modelling。 

3. 贡献点：

   1. 提出了Multi-Tasks训练框架/方法，可以防止数据泄漏(因为图片互相引用)：直接删除所有任务中出现在其他测试集合中的Image。然后约11%reduction average cross datasets
   2. 在12个数据集，4个任务上进行了训练。270M -> 12M 模型参数
   3. 首先12-in-1训练，然后进行fine-tune可以达到平均涨点2%

4. 解决多任务训练问题：任务大小和难度都不同

   1. 使用三个技巧解决：Pretrain + DSG + Round-Robin Batch Sampling
   2. 海量超参数问题：single task pretrain 然后选择最合适的，然后组合就行。
   3. 线性 Warmup 很有用
   4. LossScaling：公共模型参数学习率设置为最小的学习率，然后Loss进行Scale来适应对应的学习率。

5. 如何处理Visual + Textual关系：(Grounding)
   $$
   Score = W_r * h_{vi}
   $$

##### SCAN

<span id='scan' >label:scan</span>

1. SCAN的简写是： Stacked Cross Attention Network
2. SCAN框架是隐空间模型。
3. SCAN视觉模型：Bottom-up attention network。语言模型GRU。
4. SCAN的Grounding框架：首先计算word-region scores，然后通过第一步的计算Image-Sentence scores。
5. Image-Text Stacked Cross Attention：
   1. ith region：计算出 vi 和 ai，其中ai是image-region对words的加权和。
   2. 通过cos(ai, vi)表示Region-Sentence Sim
6. Text-Image Stacked Cross Attention：

## NLP - WordNet

#### 导图

展示各类的NLP embedding模型的方法。

```mermaid
graph TD
Root[NLP Embedding 模型] --分类1--- A[word-level模型]
Root --分类2--- B[character-level模型]
A ---- C[Word2Vec:分为CBOW和SG <br>还包含负采样代替softmax的方式]
A ---- D[GloVe:<br>学习目标是单词emb的点乘<br>等于co-occurrence proba]
A ---- E[Bag Of Word:<br>]
A ---- F[Hierarchical Embeddings Semantic:<br>使用WordNet来训练的一种Emb]

click F "#SJE" "来自SJE方法"
```

## Knowledge Graph of Zero-Shot Learning(ZSL)

#### 导图

```mermaid
graph TD
Root[Zero-Shot] --定义--> A(常用于识别在训练集合中没有出现过的物体,可以是由于缺失label信息等导致的)
Root --应用场景----> B(训练集合/测试集合/Eval集合中的Labels是disjoint的)
Root --识别算法----> C(sdf)
Root --如何解决zero-shot------> D(1.SJE方法中使用label之间的side information来进行zero-shot推理)
```

#### 参考文献：

1. [Zero-Shot Learning Survey 2017年](https://arxiv.org/pdf/1707.00600.pdf)

## 论文阅读模版

变主动为被动，由被动传授转化为主动获取型方法，先定义主要问题，然后搜索答案，其次搜集启发，最后拾遗。

对于阅读论文来说，我们定义主要问题的一些模版：

1. 这个论文的---年限/会议/单位是什么？
2. 这个论文使用的数据是什么？
3. 这个论文的任务是什么？
4. 这个任务当前的难点是什么，和自己的任务有什么相关性。
5. 这个论文的最大创新点是哪些？
6. 每个创新点分别是在模型的那些步骤得到解决的？
7. 这个模型的训练流程？



## CVPR 2017

1. 这个论文的---年限/会议/单位是什么？
2. 这个论文使用的数据是什么？
3. 这个论文的任务是什么？
4. 这个任务当前的难点是什么，和自己的任务有什么相关性。
5. 这个论文的最大创新点是哪些？
6. 每个创新点分别是在模型的那些步骤得到解决的？
7. 这个模型的训练流程？

---

1. 当前的方法使用 Attribute？这些 Attribute 怎么使用的，哪些文章用的？

2. Attribute的缺点是什么？

   一个是：Attribute如果想要进行fine-grainned 分类需要更多的标签

   二个是：Attribute如果难以提供NLP接口？

3. 这个文章使用什么解决了这个缺点？

4. 这个文章基于的文章是：

   [Evaluation of Output Embeddings for Fine-Grained Image Classi- fication. In CVPR, 2015.]()   --- <span id='SJE'>SJE</span>

5. 主要贡献：

   1. 提出了两个数据集。提供了额外的caption数据：鸟+花
   2. 对4中的文章进行了拓展，可以端到端，可以用于zero-shot retrieval任务。
   3. 对比：字符Emb模型 + wordEmb模型 + 混合CNN-LSTM NLP Emb模型。第三个是自己提出的。

6. Extension of Structured joint embedding 模型是什么？这个模型缺点？这个模型针对什么任务？如何改进的？

   ESJE方法使用的是Attribute等提取的离散文本信息来作为retrievel，28.4%弱监督精度相比于 50%的Human Attribute强监督。

   <img src="/xkblog/public/img/截屏2021-03-15 下午8.39.06.png" alt="截屏2021-03-15 下午8.39.06" style="zoom:50%;" />

7. Zero-Shot 场景是啥？ 

   答： Zero-Shot场景就是，target classes 在 Train Val Test 3个集合上都是disjoint。然后Zero-Shot Learning可以通过挖掘classes之间的关系来判断zero-shot classes

8. [SJE](#SJE) 方法结果表示：1. 视觉特征中，GoogleNet比FV和AlexNet特征好。 2. Unsupervised特征中，text-based unspervised效果最好，wordnet其实效果一般。 3. human attribute中连续特征比离散特征好。 4. 多特征融合可以有cnc和cmb两种方式，其中各有千秋，可以作为调参方式。 5. 联合使用Word2Vec和fine-grained document来进行若监督训练output embedding（class embedding）

9. [SJE](#SJE) 实验过程有时间可以学习一下。构建一个 zero-shot learning的branch mark就好了。因为高水平CV会议一般需要多个数据集验证。更好的是多个任务验证方法的有效性。